{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from extraction import extract_patches\n",
    "from reconstruction import perform_voting, generate_indexes\n",
    "\n",
    "from model import Multimodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 :\n",
      "2 :\n",
      "3 :\n",
      "4 :\n",
      "5 :\n",
      "6 :\n",
      "7 :\n",
      "8 :\n",
      "9 :\n",
      "10 :\n",
      "11 :\n",
      "12 :\n",
      "13 :\n",
      "14 :\n",
      "15 :\n",
      "16 :\n",
      "17 :\n",
      "18 :\n",
      "19 :\n",
      "20 :\n",
      "21 :\n",
      "22 :\n",
      "23 :\n",
      "24 :\n",
      "25 :\n",
      "26 :\n",
      "27 :\n",
      "28 :\n",
      "29 :\n",
      "30 :\n",
      "31 :\n",
      "32 :\n",
      "33 :\n",
      "34 :\n",
      "35 :\n",
      "36 :\n",
      "37 :\n",
      "38 :\n",
      "39 :\n",
      "40 :\n",
      "41 :\n",
      "42 :\n",
      "43 :\n",
      "44 :\n",
      "45 :\n",
      "46 :\n",
      "47 :\n",
      "48 :\n",
      "49 :\n",
      "50 :\n",
      "51 :\n",
      "52 :\n",
      "53 :\n",
      "54 :\n",
      "55 :\n",
      "56 :\n",
      "57 :\n",
      "58 :\n",
      "59 :\n",
      "60 :\n",
      "61 :\n",
      "62 :\n",
      "63 :\n",
      "64 :\n",
      "65 :\n",
      "66 :\n",
      "67 :\n",
      "68 :\n",
      "69 :\n",
      "70 :\n",
      "71 :\n",
      "72 :\n",
      "73 :\n",
      "74 :\n",
      "75 :\n",
      "80 :\n",
      "81 :\n",
      "82 :\n",
      "83 :\n",
      "84 :\n",
      "85 :\n",
      "86 :\n",
      "87 :\n",
      "88 :\n",
      "89 :\n",
      "90 :\n",
      "91 :\n",
      "92 :\n",
      "93 :\n",
      "94 :\n",
      "95 :\n",
      "96 :\n",
      "97 :\n",
      "98 :\n",
      "99 :\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nibabel as nib\n",
    "\n",
    "from medical_data import cdr_info, nwbv_info, diff_info\n",
    "\n",
    "orig_pattern = 'datasets/OASIS/OAS2_RAW_PART1/OAS2_00{0:02}_MR{1}/RAW/mpr-1_sstr_susan_matched.nii.gz'\n",
    "prob_pattern = 'datasets/OASIS/OAS2_RAW_PART1/OAS2_00{0:02}_MR{1}/RAW/mpr-1_sstr_susan_pve_{2}.nii.gz'\n",
    "\n",
    "step = (32, 32)\n",
    "threshold = np.int32(0.50 * np.prod(curr_patch_shape[:]))\n",
    "seg_train = np.empty((0, 1, ) + curr_patch_shape)\n",
    "ref_train = np.empty((0, 2, ) + curr_patch_shape)\n",
    "out_train = np.empty((0, 1, ) + curr_patch_shape)\n",
    "for i in np.append(range(1, 76), range(80, 100)) :\n",
    "    print '{} :'.format(i),\n",
    "    \n",
    "    for j in range(1, 6) :\n",
    "        ref_filename = orig_pattern.format(i, j)\n",
    "\n",
    "        if not os.path.exists(ref_filename) :\n",
    "            continue\n",
    "        \n",
    "        is_last_scan = True\n",
    "        for k in range(j+1, 6) :\n",
    "            mov_filename = orig_pattern.format(i, k)\n",
    "\n",
    "            if os.path.exists(mov_filename) :\n",
    "                is_last_scan = False\n",
    "                break\n",
    "                \n",
    "        if is_last_scan :\n",
    "            break\n",
    "            \n",
    "        if diff_info['OAS2_00{0:02}_MR{1}'.format(i, k)] == 0 :\n",
    "            break\n",
    "\n",
    "        volume_init = nib.load(ref_filename).get_data()\n",
    "\n",
    "        mask_patches = extract_patches(volume_init != 0, (1, ) + curr_patch_shape, (1, ) + step)\n",
    "\n",
    "        useful_patches = np.sum(mask_patches, axis=(1, 2, 3)) > threshold\n",
    "\n",
    "        N = np.sum(useful_patches)\n",
    "\n",
    "        volume_init = (volume_init - volume_init.mean()) / volume_init.std()\n",
    "\n",
    "        ref_patches = extract_patches(volume_init, (1, ) + curr_patch_shape, (1, ) + step)\n",
    "        ref_patches = ref_patches[useful_patches].reshape((-1, 1, ) + curr_patch_shape)\n",
    "\n",
    "        new_patches = np.empty((N, 1, ) + curr_patch_shape)\n",
    "        for a in range(1) :\n",
    "            prob_filename = prob_pattern.format(i, j, a)\n",
    "            prob_mask_init = nib.load(prob_filename).get_data()\n",
    "            prob_patches = extract_patches(prob_mask_init, (1, ) + curr_patch_shape, (1, ) + step)\n",
    "            prob_patches = prob_patches[useful_patches].reshape((-1, ) + curr_patch_shape)\n",
    "\n",
    "            new_patches[:, a] = prob_patches\n",
    "\n",
    "            del prob_patches\n",
    "        \n",
    "        ref_train = np.vstack((np.hstack((ref_patches, new_patches)), ref_train)) ##\n",
    "        del volume_init, ref_patches, new_patches\n",
    "\n",
    "        #####################\n",
    "        volume_init = nib.load(mov_filename).get_data()\n",
    "        volume_init = (volume_init - volume_init.mean()) / volume_init.std()\n",
    "\n",
    "        mov_patches = extract_patches(volume_init, (1, ) + curr_patch_shape, (1, ) + step)\n",
    "        mov_patches = mov_patches[useful_patches].reshape((-1, 1, ) + curr_patch_shape)\n",
    "\n",
    "        out_train = np.vstack((mov_patches, out_train)) ##\n",
    "        del volume_init, mov_patches\n",
    "\n",
    "        new_patches = np.empty((N, 1, ) + curr_patch_shape)\n",
    "        for a in range(1) :\n",
    "            prob_filename = prob_pattern.format(i, k, a)\n",
    "            prob_mask_init = nib.load(prob_filename).get_data()\n",
    "            prob_patches = extract_patches(prob_mask_init, (1, ) + curr_patch_shape, (1, ) + step)\n",
    "            prob_patches = prob_patches[useful_patches].reshape((-1, ) + curr_patch_shape)\n",
    "\n",
    "            new_patches[:, a] = prob_patches\n",
    "\n",
    "            del prob_patches\n",
    "\n",
    "        seg_train = np.vstack((new_patches, seg_train)) ##\n",
    "        del new_patches\n",
    "\n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latent dimensions: 32\n",
      "Fuse latent representations using max\n",
      "making output: Tensor(\"enc_T1_act9_5/add:0\", shape=(?, 32, 128, 128), dtype=float32) Tensor(\"dec_Gen_15/dec_Gen_act5/add:0\", shape=(?, 1, 128, 128), dtype=float32) em_0_dec_Gen\n",
      "making output: Tensor(\"enc_seg_act9_5/add:0\", shape=(?, 32, 128, 128), dtype=float32) Tensor(\"dec_Gen_16/dec_Gen_act5/add:0\", shape=(?, 1, 128, 128), dtype=float32) em_1_dec_Gen\n",
      "making output: Tensor(\"combined_em_5/Maximum:0\", shape=(?, 32, 128, 128), dtype=float32) Tensor(\"dec_Gen_17/dec_Gen_act5/add:0\", shape=(?, 1, 128, 128), dtype=float32) em_2_dec_Gen\n",
      "making output: em_concat Tensor(\"em_concat_5/concat:0\", shape=(?, 2, ?), dtype=float32) em_concat_5/concat:0\n",
      "all outputs:  [u'em_0_dec_Gen_5/add:0', u'em_1_dec_Gen_5/add:0', u'em_2_dec_Gen_5/add:0', u'em_concat_5/concat:0', u'em_fused_5/ExpandDims:0']\n",
      "output dict:  {'em_concat': <function embedding_distance at 0x7f27ef5c6aa0>, 'em_fused': <function embedding_distance at 0x7f27ef5c6aa0>, 'em_1_dec_Gen': <function mean_absolute_error at 0x7f28ba85d578>, 'em_0_dec_Gen': <function mean_absolute_error at 0x7f28ba85d578>, 'em_2_dec_Gen': <function mean_absolute_error at 0x7f28ba85d578>}\n",
      "loss weights:  {'em_concat': 1.0, 'em_fused': 0.0, 'em_1_dec_Gen': 1.0, 'em_0_dec_Gen': 1.0, 'em_2_dec_Gen': 1.0}\n"
     ]
    }
   ],
   "source": [
    "curr_patch_shape = (128, 128)\n",
    "input_modalities = ['T1', 'seg']\n",
    "output_modalities = ['Gen']\n",
    "output_weights = {'Gen' : 1.0, 'concat' : 1.0}\n",
    "latent_dim = 32\n",
    "channels = [2, 1]\n",
    "use_dropout = [False, False]\n",
    "patch_shape = curr_patch_shape\n",
    "scale = 1\n",
    "a_model = Multimodel(\n",
    "    input_modalities, output_modalities, output_weights, latent_dim, channels, patch_shape, use_dropout, scale)\n",
    "a_model.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6493 samples, validate on 2783 samples\n",
      "Epoch 1/20\n",
      "6493/6493 [==============================] - 111s 17ms/step - loss: 0.8180 - em_0_dec_Gen_loss: 0.2018 - em_1_dec_Gen_loss: 0.3972 - em_2_dec_Gen_loss: 0.1996 - em_concat_loss: 0.0195 - em_fused_loss: 0.0000e+00 - val_loss: 0.4846 - val_em_0_dec_Gen_loss: 0.1419 - val_em_1_dec_Gen_loss: 0.2087 - val_em_2_dec_Gen_loss: 0.1234 - val_em_concat_loss: 0.0107 - val_em_fused_loss: 0.0000e+00\n",
      "Epoch 2/20\n",
      "6493/6493 [==============================] - 96s 15ms/step - loss: 0.4436 - em_0_dec_Gen_loss: 0.1388 - em_1_dec_Gen_loss: 0.1801 - em_2_dec_Gen_loss: 0.1165 - em_concat_loss: 0.0082 - em_fused_loss: 0.0000e+00 - val_loss: 0.4468 - val_em_0_dec_Gen_loss: 0.1380 - val_em_1_dec_Gen_loss: 0.1884 - val_em_2_dec_Gen_loss: 0.1139 - val_em_concat_loss: 0.0064 - val_em_fused_loss: 0.0000e+00\n",
      "Epoch 3/20\n",
      "6493/6493 [==============================] - 96s 15ms/step - loss: 0.4027 - em_0_dec_Gen_loss: 0.1347 - em_1_dec_Gen_loss: 0.1568 - em_2_dec_Gen_loss: 0.1058 - em_concat_loss: 0.0054 - em_fused_loss: 0.0000e+00 - val_loss: 0.4294 - val_em_0_dec_Gen_loss: 0.1373 - val_em_1_dec_Gen_loss: 0.1781 - val_em_2_dec_Gen_loss: 0.1095 - val_em_concat_loss: 0.0046 - val_em_fused_loss: 0.0000e+00\n",
      "Epoch 4/20\n",
      "6493/6493 [==============================] - 97s 15ms/step - loss: 0.3808 - em_0_dec_Gen_loss: 0.1321 - em_1_dec_Gen_loss: 0.1443 - em_2_dec_Gen_loss: 0.1002 - em_concat_loss: 0.0041 - em_fused_loss: 0.0000e+00 - val_loss: 0.4311 - val_em_0_dec_Gen_loss: 0.1379 - val_em_1_dec_Gen_loss: 0.1799 - val_em_2_dec_Gen_loss: 0.1094 - val_em_concat_loss: 0.0039 - val_em_fused_loss: 0.0000e+00\n",
      "Epoch 5/20\n",
      "6493/6493 [==============================] - 96s 15ms/step - loss: 0.3681 - em_0_dec_Gen_loss: 0.1306 - em_1_dec_Gen_loss: 0.1370 - em_2_dec_Gen_loss: 0.0971 - em_concat_loss: 0.0034 - em_fused_loss: 0.0000e+00 - val_loss: 0.4205 - val_em_0_dec_Gen_loss: 0.1362 - val_em_1_dec_Gen_loss: 0.1734 - val_em_2_dec_Gen_loss: 0.1078 - val_em_concat_loss: 0.0031 - val_em_fused_loss: 0.0000e+00\n",
      "Epoch 6/20\n",
      "6493/6493 [==============================] - 96s 15ms/step - loss: 0.3509 - em_0_dec_Gen_loss: 0.1258 - em_1_dec_Gen_loss: 0.1297 - em_2_dec_Gen_loss: 0.0926 - em_concat_loss: 0.0029 - em_fused_loss: 0.0000e+00 - val_loss: 0.4170 - val_em_0_dec_Gen_loss: 0.1356 - val_em_1_dec_Gen_loss: 0.1715 - val_em_2_dec_Gen_loss: 0.1071 - val_em_concat_loss: 0.0027 - val_em_fused_loss: 0.0000e+00\n",
      "Epoch 7/20\n",
      "6493/6493 [==============================] - 96s 15ms/step - loss: 0.3355 - em_0_dec_Gen_loss: 0.1216 - em_1_dec_Gen_loss: 0.1230 - em_2_dec_Gen_loss: 0.0884 - em_concat_loss: 0.0025 - em_fused_loss: 0.0000e+00 - val_loss: 0.4243 - val_em_0_dec_Gen_loss: 0.1387 - val_em_1_dec_Gen_loss: 0.1713 - val_em_2_dec_Gen_loss: 0.1121 - val_em_concat_loss: 0.0023 - val_em_fused_loss: 0.0000e+00\n",
      "Epoch 8/20\n",
      "6493/6493 [==============================] - 96s 15ms/step - loss: 0.3287 - em_0_dec_Gen_loss: 0.1188 - em_1_dec_Gen_loss: 0.1211 - em_2_dec_Gen_loss: 0.0867 - em_concat_loss: 0.0022 - em_fused_loss: 0.0000e+00 - val_loss: 0.4137 - val_em_0_dec_Gen_loss: 0.1363 - val_em_1_dec_Gen_loss: 0.1683 - val_em_2_dec_Gen_loss: 0.1070 - val_em_concat_loss: 0.0021 - val_em_fused_loss: 0.0000e+00\n",
      "Epoch 9/20\n",
      "6493/6493 [==============================] - 96s 15ms/step - loss: 0.3097 - em_0_dec_Gen_loss: 0.1131 - em_1_dec_Gen_loss: 0.1135 - em_2_dec_Gen_loss: 0.0811 - em_concat_loss: 0.0020 - em_fused_loss: 0.0000e+00 - val_loss: 0.4225 - val_em_0_dec_Gen_loss: 0.1394 - val_em_1_dec_Gen_loss: 0.1703 - val_em_2_dec_Gen_loss: 0.1110 - val_em_concat_loss: 0.0019 - val_em_fused_loss: 0.0000e+00\n",
      "Epoch 10/20\n",
      "6493/6493 [==============================] - 96s 15ms/step - loss: 0.3082 - em_0_dec_Gen_loss: 0.1118 - em_1_dec_Gen_loss: 0.1134 - em_2_dec_Gen_loss: 0.0812 - em_concat_loss: 0.0019 - em_fused_loss: 0.0000e+00 - val_loss: 0.4149 - val_em_0_dec_Gen_loss: 0.1368 - val_em_1_dec_Gen_loss: 0.1683 - val_em_2_dec_Gen_loss: 0.1079 - val_em_concat_loss: 0.0018 - val_em_fused_loss: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f27e12ec9d0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "patience = 2\n",
    "\n",
    "stopper = EarlyStopping(patience=patience)\n",
    "checkpointer = ModelCheckpoint('model.h5', save_best_only=True, save_weights_only=True)\n",
    "\n",
    "N = len(ref_train)\n",
    "a_model.model.fit(\n",
    "    [ref_train, seg_train],\n",
    "    [out_train, out_train, out_train, np.empty((N, 2, 0)), np.empty((N, 1, 0))],\n",
    "    validation_split=0.3, epochs=20,\n",
    "    callbacks=[checkpointer, stopper])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a_model.model.load_weights('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76\n",
      "1280/1280 [==============================] - 10s 8ms/step\n",
      "78\n",
      "1280/1280 [==============================] - 10s 8ms/step\n",
      "79\n",
      "1280/1280 [==============================] - 10s 8ms/step\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "import os\n",
    "import nibabel as nib\n",
    "import SimpleITK as sitk\n",
    "\n",
    "orig_pattern = 'datasets/OASIS/OAS2_RAW_PART1/OAS2_00{0:02}_MR{1}/RAW/mpr-1_sstr_susan_matched.nii.gz'\n",
    "prob_pattern = 'datasets/OASIS/OAS2_RAW_PART1/OAS2_00{0:02}_MR{1}/RAW/mpr-1_sstr_susan_pve_{2}.nii.gz'\n",
    "\n",
    "step = (32, 32)\n",
    "for i in range(76, 80) :\n",
    "    ref_filename = orig_pattern.format(i, 1)\n",
    "    mov_filename = orig_pattern.format(i, 3)\n",
    "    if not (os.path.exists(ref_filename) and os.path.exists(mov_filename)) :\n",
    "        continue\n",
    "    \n",
    "    print i\n",
    "\n",
    "    volume_data = nib.load(ref_filename)\n",
    "    volume_actual = volume_data.get_data()\n",
    "    volume_actual = (volume_actual - volume_actual.mean()) / volume_actual.std()\n",
    "    ref_patches = extract_patches(volume_actual, (1, ) + curr_patch_shape, (1, ) + step)\n",
    "    ref_patches = ref_patches.reshape((-1, 1, ) + curr_patch_shape)\n",
    "    \n",
    "    N = len(ref_patches)\n",
    "\n",
    "    old_patches = np.empty((N, 1, ) + curr_patch_shape)\n",
    "    for k in range(1) :\n",
    "        filename = prob_pattern.format(i, 1, k)\n",
    "        prob_mask_init = nib.load(filename).get_data()\n",
    "        prob_patches = extract_patches(prob_mask_init, (1, ) + curr_patch_shape, (1, ) + step)\n",
    "        prob_patches = prob_patches.reshape((-1, ) + curr_patch_shape)\n",
    "\n",
    "        old_patches[:, k] = prob_patches\n",
    "\n",
    "        del prob_patches\n",
    "    \n",
    "    new_patches = np.empty((N, 1, ) + curr_patch_shape)\n",
    "    for k in range(1) :\n",
    "        filename = prob_pattern.format(i, 3, k)\n",
    "        prob_mask_init = nib.load(filename).get_data()\n",
    "        prob_patches = extract_patches(prob_mask_init, (1, ) + curr_patch_shape, (1, ) + step)\n",
    "        prob_patches = prob_patches.reshape((-1, ) + curr_patch_shape)\n",
    "\n",
    "        new_patches[:, k] = prob_patches\n",
    "\n",
    "        del prob_patches\n",
    "    \n",
    "    pred = a_model.model.predict([np.hstack((ref_patches, old_patches)), new_patches], verbose=1)[2]\n",
    "\n",
    "    volume = perform_voting(pred.reshape((-1, 1, ) + curr_patch_shape), (1, ) + curr_patch_shape, (256, 256, 128), (1, ) + step)\n",
    "    \n",
    "    volume_data = nib.load(mov_filename)\n",
    "    volume_actual = volume_data.get_data()\n",
    "    volume = volume * volume_actual.std() + volume_actual.mean()\n",
    "    \n",
    "    volume = np.multiply(volume_data.get_data() != 0, volume)\n",
    "\n",
    "    nib.save(nib.Nifti1Image(volume, volume_data.affine), 'result_{}.nii.gz'.format(i))\n",
    "    \n",
    "    del new_patches, ref_patches\n",
    "    \n",
    "    res = sitk.ReadImage('result_{}.nii.gz'.format(i))\n",
    "\n",
    "    caster = sitk.CastImageFilter()\n",
    "    caster.SetOutputPixelType(res.GetPixelID())\n",
    "\n",
    "    orig = caster.Execute(sitk.ReadImage(orig_pattern.format(i, 1)))\n",
    "\n",
    "    matcher = sitk.HistogramMatchingImageFilter()\n",
    "    matcher.SetNumberOfHistogramLevels(1024)\n",
    "    matcher.SetNumberOfMatchPoints(15)\n",
    "    matched = matcher.Execute(res, orig)\n",
    "\n",
    "    sitk.WriteImage(matched, 'result_cor_{}.nii.gz'.format(i))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
